{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic calcium imaging analysis using Jupyter, Spark and Thunder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import h5py\n",
    "import os, sys\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add folder utils to the Python path\n",
    "utils_dir = os.path.join(os.getcwd(), 'utils')\n",
    "sys.path.append(utils_dir) # add folder to Python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "layout": {
       "col": 0,
       "height": 4,
       "row": 0,
       "width": 4
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# starting Spark depends on where the notebook is running\n",
    "# choose 'local' or 'openstack'\n",
    "nbBackend = 'local'\n",
    "print \"Running notebook on \" + nbBackend + \" backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from setupSpark import initSpark\n",
    "sc = initSpark(nbBackend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add py-files in this folder to the SparkContext \n",
    "# (this allows that all required files are available on all workers)\n",
    "for filename in os.listdir(utils_dir):\n",
    "    if filename.endswith('.py'):\n",
    "        sc.addPyFile(os.path.join(utils_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# full path to HDF5 data file\n",
    "directory = '/Users/Henry/polybox/Data_temp/NeuroPipeline/LEC_Data'\n",
    "# select H5 file\n",
    "h5file = 'Monyer_Leitner_F296_spot01.h5'\n",
    "h5file = directory + os.sep + h5file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# example: read one neuron timeseries (all trials)\n",
    "ix = 0 # indexing is zero-based\n",
    "from NeuroH5Utils import readPixel_map\n",
    "x, result = readPixel_map(ix, h5file, dim=1, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# example: read a single timepoint for all neurons (all trials)\n",
    "ix = 100\n",
    "from NeuroH5Utils import readPixel_map\n",
    "x, result = readPixel_map(ix, h5file, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# obtain further information about the dataset (size, sampling rate, number of trials)\n",
    "from NeuroH5Utils import getFileInfo\n",
    "dsetSz, sampF, nTrials = getFileInfo(h5file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# create the RDD\n",
    "from NeuroH5Utils import convert2RDD\n",
    "numPartitions = 10 # how many partitions?\n",
    "rdd = convert2RDD(sc, h5file, numPartitions=numPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# use count to force loading of the data, i.e. access every element once\n",
    "# count returns number of elements in the RDD (i.e. dsetSz[0])\n",
    "nNeurons = rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# get time series from first roi to compute timepoints\n",
    "s  = np.asarray(rdd.lookup(0))\n",
    "# create a time array t\n",
    "t = (np.linspace(1, len(s[0]), len(s[0]))) / sampF \n",
    "nTimepoints = len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# return a specific Roi as Python list and plot\n",
    "roi = 0\n",
    "s = rdd.lookup(roi) # returns a list\n",
    "s = np.asarray(s) # convert to np array (actually not required for plotting)\n",
    "plt.plot(t, s[0]);\n",
    "plt.xlim((0, np.max(t)));\n",
    "plt.xlabel('Time / s');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# convert the Spark RDD into a Thunder Series object\n",
    "# this allows us to make use of the functions available for Series objects in the Thunder library\n",
    "import thunder as td\n",
    "# thunder fromrdd expects a key-value pair where the key is a tuple representing the index\n",
    "# here keys are ints --> wrap them in a single-element tuple\n",
    "series = td.series.fromrdd(rdd.map(lambda kv: ((kv[0],), kv[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# the index of a series allows us to store some important information for each timepoint, for example the trial number\n",
    "import numpy.matlib\n",
    "for iTrial in range(nTrials):\n",
    "    v = np.matlib.repmat(iTrial, len(t)/nTrials, 1)\n",
    "    if iTrial == 0:\n",
    "        index = v\n",
    "    else:\n",
    "        index = np.append(index, v)\n",
    "series.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# Example: select some ROIs above a certain mean intensity threshold and plot them\n",
    "examples = series.filter(lambda x: x.mean() > 10).sample(5).toarray()\n",
    "plt.plot(t, examples.T);\n",
    "plt.xlim((0, np.max(t)));\n",
    "plt.xlabel('Time / s');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# Example: Compute mean / standard deviation of each Roi\n",
    "series_mean = series.map(lambda x: x.mean()).flatten().toarray()\n",
    "series_sd = series.map(lambda x: x.std()).flatten().toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# Scatter plot of Roi mean vs. SD\n",
    "plt.scatter(series_mean, series_sd)\n",
    "plt.xlabel('Roi Mean');\n",
    "plt.ylabel('Roi SD');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# import stimulus data\n",
    "from NeuroH5Utils import getStimData\n",
    "stimData, stimNames = getStimData(h5file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot timeseries for all neurons stacked and with odor stimuli indicated\n",
    "# Extract data directly from Spark RDD because it's faster than going via Thunder\n",
    "fig = plt.figure(figsize=(8,4)) # increase figsize to (20,10) to improve visibility\n",
    "offset = 0\n",
    "for iNeuron in range(nNeurons):\n",
    "    plotTrace = rdd.lookup(iNeuron) # returns a list\n",
    "    plotTrace = np.asarray(plotTrace) # convert to np array (actually not required for plotting)\n",
    "    plotTrace = plotTrace[0] - min(plotTrace[0]) + offset\n",
    "    offset = max(plotTrace)\n",
    "    plt.plot(t, plotTrace)\n",
    "for iTimepoint in range(len(stimData)):\n",
    "    if stimData[iTimepoint]:\n",
    "        tStim = t[iTimepoint]\n",
    "        plt.plot((tStim, tStim), (0, offset), 'k--')\n",
    "plt.xlabel('Time [s]', fontsize=18)\n",
    "plt.ylim((0, offset))\n",
    "plt.xlim((0, np.max(t)))\n",
    "ax = fig.gca()\n",
    "plt.setp(ax.get_xticklabels(), fontsize=16)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=16)\n",
    "plt.show()\n",
    "# save the figure if required\n",
    "# plt.savefig('Timeseries_AllStim.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# peri-stimulus plots - average traces for each stimulus per neuron\n",
    "# TODO: remove for-loop?\n",
    "from CalciumAnalysisUtils import psAnalysis\n",
    "\n",
    "# select time interval to plot (in frames)\n",
    "baseFrames = 10\n",
    "evokedFrames = 100\n",
    "\n",
    "# select Rois to plot\n",
    "# TODO: bug when selecting only 1 Roi\n",
    "roisToPlot = (0, 16, 25, 26)\n",
    "# Or select all neurons\n",
    "# roisToPlot = range(nNeurons)\n",
    "\n",
    "# compute peri-stimulus data for all neurons from the Spark RDD\n",
    "# this creates a new RDD called psData\n",
    "psData = rdd.map(lambda (k, v): (k, psAnalysis(v, stimData, (baseFrames, evokedFrames))))\n",
    "psData = psData.partitionBy(numPartitions).cache()\n",
    "\n",
    "# now create the plot\n",
    "fig = plt.figure(figsize=(20,20)) # (20, 200) for full dataset, otherwise fewer rows\n",
    "splotCounter = 1\n",
    "for ix, iRoi in enumerate(roisToPlot):\n",
    "    iRoi_data = np.asarray(psData.lookup(iRoi))\n",
    "    psDataByStim = iRoi_data[0]\n",
    "    # same y range for all stims\n",
    "    minY = min([ np.min(x) for x in psDataByStim ])\n",
    "    maxY = max([ np.max(x) for x in psDataByStim ])\n",
    "    # plot for each stimulus\n",
    "    for ix2, iStim in enumerate(psDataByStim):\n",
    "        meanData = np.mean(iStim,axis=0)\n",
    "        semData = np.std(iStim,axis=0) / np.sqrt(np.shape(iStim)[0])\n",
    "        tPs = (np.linspace(0, evokedFrames, meanData.size)-baseFrames)/sampF\n",
    "        plt.subplot(len(roisToPlot), len(psDataByStim), splotCounter)\n",
    "        splotCounter = splotCounter + 1\n",
    "        plt.fill_between(tPs, meanData-semData, meanData+semData, alpha=0.2)\n",
    "        plt.plot(tPs, meanData)\n",
    "        plt.plot((0,0), (minY, maxY), 'k--')\n",
    "        plt.xlim((min(tPs), max(tPs)))\n",
    "        plt.ylim((minY, maxY))\n",
    "        if ix == 0:\n",
    "            plt.title(stimNames[ix2+1])\n",
    "        if ix2 == 0:\n",
    "            plt.ylabel('%DFF Roi {0}'.format(iRoi+1))\n",
    "plt.show()\n",
    "# plt.savefig('PsPlot_AllStims.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# get the reference image for a single trial and display it\n",
    "from NeuroH5Utils import getReferenceImage\n",
    "from showit import image\n",
    "trial = 0 # specify trial (0 based indexing)\n",
    "refImage = getReferenceImage(h5file, trial=trial)\n",
    "image(refImage, clim=(0,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "urth": {
     "dashboard": {
      "hidden": true,
      "layout": {}
     }
    }
   },
   "outputs": [],
   "source": [
    "# setup another RDD, this time parallelizing across time points \n",
    "# thus, one RDD element contains the activity of all neurons for a particular time point\n",
    "from NeuroH5Utils import convert2RDD\n",
    "numPartitions = 100 # how many partitions?\n",
    "rdd_time = convert2RDD(sc, h5file, numPartitions=numPartitions, dim=2)\n",
    "# TODO: split by trials\n",
    "# use count to force loading of the data, i.e. access every element once\n",
    "rdd_time.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "urth": {
   "dashboard": {
    "cellMargin": 10,
    "defaultCellHeight": 20,
    "layout": "grid",
    "maxColumns": 12
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning analysis of calcium imaging data with Spark\n",
    "Statistical learning approaches are multivariate analysis techniques that allow the quantification of information content about a stimulus or behaviour in a population of neurons. Essentially, we try to decode the presented stimulus or observed behaviour directly from the activity level of the neurons. This is done by fitting a model to a part of the data set (the training set), similar to what was done in the Regression tutorial. To avoid over-fitting, the derived model is then applied to an independent data set (the test set). We can iteratively leave out different parts of our data in a procedure called cross-validation. Machine learning techniques are widely used for data mining in many different fields and consequently Spark provides highly optimized routines for many different algorithms. Here, we will explore the use of a Random Forrest algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial setup & data import\n",
    "This section is largely identical to the first part of [Tutorial_Basics](Tutorial_Basics.ipynb). To run it all in one go, you can select the next section heading ([Prepare stimulus data](#prep_stim)) and choose Cell --> Run All Above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import h5py\n",
    "import os, sys\n",
    "import seaborn as sns\n",
    "\n",
    "# Set figure style options for Seaborn\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "# show figure in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add folder 'utils' to the Python path\n",
    "# this folder contains custom written code that is required for data import and analysis\n",
    "utils_dir = os.path.join(os.getcwd(), 'utils')\n",
    "sys.path.append(utils_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# starting Spark depends on where the notebook is running (local computer or OpenStack cluster)\n",
    "# choose 'local' or 'openstack'\n",
    "nbBackend = 'openstack'\n",
    "print \"Running notebook on \" + nbBackend + \" backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize Spark\n",
    "# returns the SparkContext object 'sc' which tells Spark how to access the cluster\n",
    "from setupSpark import initSpark\n",
    "sc = initSpark(nbBackend)\n",
    "sc.setLogLevel('WARN') # only show most relevant output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add Python files in 'utils' folder to the SparkContext \n",
    "# this is required so that all files are available on all the cluster workers\n",
    "for filename in os.listdir(utils_dir):\n",
    "    if filename.endswith('.py'):\n",
    "        sc.addPyFile(os.path.join(utils_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# full path to directory containing HDF5 files\n",
    "directory = '/home/ubuntu/example_data'\n",
    "\n",
    "# select HDF5 file\n",
    "# following files are available: \n",
    "# Monyer_Leitner_F296_spot01.h5\n",
    "# Monyer_Leitner_F397_spot01.h5\n",
    "# Monyer_Leitner_F400_spot02.h5\n",
    "# Monyer_Leitner_F400_spot04.h5\n",
    "h5file = 'Monyer_Leitner_F296_spot01.h5'\n",
    "h5file = directory + os.sep + h5file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain further information about the dataset (size, sampling rate, number of trials)\n",
    "from NeuroH5Utils import getFileInfo\n",
    "dsetSz, sampF, nTrials = getFileInfo(h5file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the Spark RDD\n",
    "from NeuroH5Utils import convert2RDD\n",
    "numPartitions = 10 # how many partitions?\n",
    "rdd = convert2RDD(sc, h5file, numPartitions=numPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # compute number of neurons, time points and time axis\n",
    "nNeurons = rdd.count()\n",
    "s  = np.asarray(rdd.lookup(0))\n",
    "t = (np.linspace(1, len(s[0]), len(s[0]))) / sampF \n",
    "nTimepoints = len(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"prep_stim\"></a>\n",
    "### Prepare stimulus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next, we import the stimulus\n",
    "from NeuroH5Utils import getStimData\n",
    "stimData, stimNames = getStimData(h5file)\n",
    "# in this tutorial, we convert the stimulus vector to another format, to allow more efficient processing\n",
    "# we use a Python dictionary which is a key-value based data type\n",
    "# in our case, the key is the stimulus ID whereas the value is an array of start indices for the respective stimulus\n",
    "# have a look at the output of this cell and this will make more sense\n",
    "uniqueStims = np.unique(stimData[stimData>1]) # ignore air\n",
    "stimDict = {}\n",
    "for iStim in uniqueStims:\n",
    "    stimDict[int(iStim)-1] = np.where(stimData==iStim)[0]\n",
    "print stimDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the advantage of using a dictionary is that we can now easily retrieve the indices for a particular stimulus:\n",
    "stimDict[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the Spark machine learning library: MLlib\n",
    "MLlib is Sparkâ€™s machine learning library. Its goal is to make practical machine learning scalable and easy. It consists of common learning algorithms and utilities, including classification, regression, clustering, collaborative filtering, dimensionality reduction, as well as lower-level optimization primitives and higher-level pipeline APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the relevant modules from MLlib\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single neuron classification analysis\n",
    "First, we apply machine learning to quantify the stimulus-specific information content in each Roi's response individually. To do this, we extract the peri-stimulus data as in [Tutorial_Basics](Tutorial_Basics.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "from CalciumAnalysisUtils import psAnalysis\n",
    "\n",
    "# select time interval around stimulus (in frames; timepoints before stimulus onset will be ignored later)\n",
    "baseFrames = 10\n",
    "evokedFrames = 100\n",
    "\n",
    "# compute PSdata for all neurons\n",
    "# note that psAnalysis ignores stims with ID = 1 (air)\n",
    "psData = rdd.map(lambda (k, v): (k, psAnalysis(v, stimData, (baseFrames, evokedFrames))))\n",
    "psData = psData.partitionBy(numPartitions).cache()\n",
    "tPs = (np.linspace(0, evokedFrames, baseFrames+evokedFrames)-baseFrames)/sampF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the data for classification analysis with Spark's MLlib. The most common data type for this in MLlib is called a Labeled Point. A labeled point is a local vector, either dense or sparse, associated with a label/response. In MLlib, labeled points are used in supervised learning algorithms, like Random Forrest. We use a double to store a label, so we can use labeled points in both regression and classification. For binary classification, a label should be either 0 (negative) or 1 (positive). For multiclass classification, labels should be class indices starting from zero: 0, 1, 2, ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here we prepare a list of LabeledPoints, one for each neuron\n",
    "# neurons2classify = nNeurons # select a subset for testing\n",
    "debug = 0 # plot selected features if debug=1\n",
    "labeledPoint_rddList = []\n",
    "# loop over all Rois\n",
    "for nCell in xrange(nNeurons):\n",
    "    nCell_lp = []\n",
    "    # get ps-data for this Roi\n",
    "    cellPsData = psData.lookup(nCell)[0]\n",
    "    # loop over stimulus types\n",
    "    for nStim in range(len(cellPsData)):\n",
    "        currentData = cellPsData[nStim]\n",
    "        # loop over trials per stimulus\n",
    "        for nTrial in range(np.shape(currentData)[0]):\n",
    "            # select data points after stimulus onset\n",
    "            v = np.squeeze(currentData[nTrial, np.where(tPs > 0)])\n",
    "            # baseline normalization (subtract mean of pre-stimulus baseline)\n",
    "            v = v - np.mean(currentData[nTrial, np.where(tPs < 0)])\n",
    "            t = tPs[np.where(tPs > 0)]\n",
    "            # feature selection using linear interpolation\n",
    "            tinterp = np.linspace(np.min(t)+0.5, np.max(t), 10)\n",
    "            vinterp = np.interp(tinterp, t, v)\n",
    "            # build the LabeledPoint list\n",
    "            nCell_lp.append(LabeledPoint(nStim, vinterp))\n",
    "            # debug: plot original and interpolated data for first trial\n",
    "            if debug and not nTrial:\n",
    "                plt.plot(t,v)\n",
    "                plt.plot(tinterp, vinterp, 'o')\n",
    "                plt.show()\n",
    "    # Parallelize for each neuron and append to RDD list\n",
    "    labeledPoint_rddList.append(sc.parallelize(nCell_lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, an example how to run the classification with one iteration and one neuron\n",
    "neuronIx = 0 # index of the neuron\n",
    "# randomSplit allows us to split the data into training and test set\n",
    "# we use 70% of the data for training and 30% for testing\n",
    "(trainingData, testData) = labeledPoint_rddList[neuronIx].randomSplit([0.7, 0.3])\n",
    "# train classifier on the training data\n",
    "# note that number of decision trees is quite low here, for performance reasons\n",
    "# in production use 50 - 100 trees\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=len(stimNames)-1, categoricalFeaturesInfo={}, \n",
    "                                     numTrees=10, featureSubsetStrategy=\"auto\", \n",
    "                                     impurity='gini', maxDepth=4, maxBins=20)\n",
    "# use the model to obtain predictions for the test data set\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "# compute prediction accuracy\n",
    "correct = labelsAndPredictions.filter(lambda (v, p): v == p).count() / float(testData.count())\n",
    "print correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next, we run the classification analysis for each Roi and several train / test splits (iterations)\n",
    "# depending on the number of iterations, this can take a whiles\n",
    "iters = 10 # number of iterations (train-test splits)\n",
    "fractCorrect = np.zeros((len(labeledPoint_rddList), iters))\n",
    "for ix, labeledPointRDD in enumerate(labeledPoint_rddList):\n",
    "    for nIter in range(iters):\n",
    "        (trainingData, testData) = labeledPointRDD.randomSplit([0.7, 0.3])\n",
    "        model = RandomForest.trainClassifier(trainingData, numClasses=len(stimNames)-1, categoricalFeaturesInfo={}, \n",
    "                                             numTrees=10, featureSubsetStrategy=\"auto\", \n",
    "                                             impurity='gini', maxDepth=4, maxBins=32)\n",
    "        predictions = model.predict(testData.map(lambda x: x.features))\n",
    "        labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "        fractCorrect[ix][nIter] = labelsAndPredictions.filter(lambda (v, p): v == p).count() / float(testData.count())\n",
    "    progress = (float(ix)/len(labeledPoint_rddList))*100\n",
    "    sys.stdout.write(\"\\r%02.0f%% completed\" % progress)\n",
    "    sys.stdout.flush()\n",
    "sys.stdout.write(\"\\rDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot classification performance for all neurons\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "plt.plot([0, nNeurons], [1.0/(len(stimNames)-1), 1.0/(len(stimNames)-1)], color='k', linestyle='--')\n",
    "plt.bar(np.linspace(1,nNeurons,nNeurons), np.mean(fractCorrect, axis=1), color='r', yerr=np.std(fractCorrect, axis=1))\n",
    "plt.ylabel('Fraction correct +- SD');\n",
    "plt.xlabel('Neuron ID');\n",
    "plt.title('Single Cell Decoding');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population classification analysis\n",
    "Next, we apply the same approach to the population response. At each time point (relative to the stimulus), the population activity vector of all neurons can be used as input for the Random Forrest algorithm. Thus, we obtain a time course of the classification accuracy relative to the stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, we write a function that performs the Random Forrest classification on a LabeledPoint RDD\n",
    "# this code is identical to the corresponding section in Single Cell classification \n",
    "# (we could recycle the function there too)\n",
    "def classify_RandomForrest(labeledPointRDD, iters, numClasses):\n",
    "    correct = np.zeros(iters)\n",
    "    labeledPointRDD = labeledPointRDD.cache()\n",
    "    for iIter in range(iters):\n",
    "        (trainingData, testData) = labeledPointRDD.randomSplit([0.7, 0.3])\n",
    "        model = RandomForest.trainClassifier(trainingData, numClasses=numClasses, categoricalFeaturesInfo={}, \n",
    "                                             numTrees=10, featureSubsetStrategy=\"auto\", \n",
    "                                             impurity='gini', maxDepth=4, maxBins=20)\n",
    "        predictions = model.predict(testData.map(lambda x: x.features))\n",
    "        labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "        correct[iIter] = labelsAndPredictions.filter(lambda (v, p): v == p).count() / float(testData.count())\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, we write a function to plot the results (to make the code more reusable)\n",
    "def plot_results(accuracy_array, psTime):\n",
    "    fig = plt.figure(figsize=(15,6))\n",
    "    plt.plot([np.min(psTime)-0.5, np.max(psTime)+1], [1.0/(len(stimNames)-1), 1.0/(len(stimNames)-1)], \n",
    "             color='k', linestyle='--')\n",
    "    plt.bar(psTime, np.mean(accuracy_array, axis=0), color='r', \n",
    "            yerr=np.std(accuracy_array, axis=0))\n",
    "    plt.ylabel('Fraction correct +- SD')\n",
    "    plt.xlabel('Timepoint')\n",
    "    plt.xlim((np.min(psTime)-0.5, np.max(psTime)+1))\n",
    "    plt.title('Population Decoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we use a more interesting way to extract the data from the RDD and convert it into a Labeled Point. This is necessary because we potentially deal with very many neurons (\"Big Data\"), so we cannot simply use rdd.lookup to extract the data (it might not fit on the Spark drivers memory). The strategy thus is to use successive RDD transformations which never have to load the full dataset into memory. First, we extract all the data points sorted according to the particular stimuli. Then, we combine these values into a new RDD with neuron index as key and the corresponding data points as values. Finally, we convert this RDD into a LabeledPoint RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function: extract points\n",
    "# this function extracts the data points of all neurons for a given stimulus / ps-timepoint combination\n",
    "def extract_points(neuron_ix, arr, stimDict, tPs):\n",
    "    points = []\n",
    "    for stim, idx_list in stimDict.iteritems():\n",
    "        for ix in idx_list:\n",
    "            points.append(((stim-1, ix+tPs), (neuron_ix, arr[ix+tPs])))\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function: add elements\n",
    "# this function takes the values from extract_points and arranges them in a new RDD\n",
    "def add_elements(arr, new):\n",
    "    neuron_ix, val = new\n",
    "    arr[neuron_ix] = val\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we define timepoints for classification around the stimulus \n",
    "# (independent population analyses are run for each timepoint)\n",
    "baseFrames = 10\n",
    "evokedFrames = 60\n",
    "# the more time points we have, the denser the sampling but the longer the analysis will take\n",
    "psTimepoints = range(-baseFrames,evokedFrames, 5)\n",
    "psTime = psTimepoints/sampF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create labeled point RDD list by RDD transformation\n",
    "# because Spark executes transformations lazily (only when needed), this runs very fast\n",
    "labeledPoint_rddList = []\n",
    "for tPs in psTimepoints:\n",
    "    point_rdd = rdd.flatMap(lambda (k,v): extract_points(k, v, stimDict, tPs))\n",
    "    point_rdd = point_rdd.aggregateByKey(np.zeros(nNeurons), add_elements, lambda a,b: a+b)\n",
    "    labeledPointRDD = point_rdd.map(lambda (k,v): LabeledPoint(k[0], v))\n",
    "    labeledPoint_rddList.append(labeledPointRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now run the classifications on the different time points\n",
    "iters = 10 # number of iterations (train-test splits)\n",
    "accuracy_array = np.zeros((iters, len(psTimepoints)))\n",
    "for ix, labeledPointRDD in enumerate(labeledPoint_rddList):\n",
    "    correct = classify_RandomForrest(labeledPointRDD, iters, len(stimNames)-1)\n",
    "    accuracy_array[:, ix] = correct\n",
    "    progress = (float(ix)/len(psTimepoints))*100\n",
    "    sys.stdout.write(\"\\r%02.0f%% completed\" % progress)\n",
    "    sys.stdout.flush()\n",
    "sys.stdout.write(\"\\rDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plot_results(accuracy_array, psTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding-window correlation analysis\n",
    "This notebook shows how to run a sliding-window correlation analysis between an ROI seed signal and the timeseries for each pixel. The output is stored as HDF5 files (one per trial) on Swift. A movie of the correlation signal for selected trials can also be generated and stored. Sliding-window correlation is a time consuming computation. It is parallelized across trials using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Python modules\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "from __future__ import print_function\n",
    "import getpass\n",
    "import tempfile\n",
    "import shutil\n",
    "import h5py\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "nbBackend = 'openstack'\n",
    "\n",
    "# add folder 'utils' to the Python path\n",
    "# this folder contains custom written code that is required for data import and analysis\n",
    "utils_dir = os.path.join(os.getcwd(), 'utils')\n",
    "sys.path.append(utils_dir)\n",
    "import SwiftStorageUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import custom-written modules\n",
    "import WidefieldDataUtils as wf\n",
    "import PickleUtils as pick\n",
    "import CalciumAnalysisUtils as calciumTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# swift file system\n",
    "swift_container = 'ariel' # specify name of container in Swift (do not use _ etc.)\n",
    "swift_provider = 'SparkTest' # in general, this should not change\n",
    "swift_basename = \"swift://\" + swift_container + \".\" + swift_provider + \"/\"\n",
    "\n",
    "# storage location of dFF data (from Widefield_Preproc_Spark_Swift notebook)\n",
    "output_folder_dff = 'dff_out'\n",
    "\n",
    "# start of name for matching files\n",
    "filename_start = '20152310_' # all files with names starting like this will be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OpenStack credentials for accessing Swift storage\n",
    "os_username = 'hluetc'\n",
    "os_tenant_name = 'helmchen.hifo.uzh'\n",
    "os_auth_url = 'https://cloud.s3it.uzh.ch:5000/v2.0'\n",
    "# provide OS password\n",
    "os_password = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put all these params in a dict for later access\n",
    "file_params = dict()\n",
    "file_params['swift_container'] = swift_container\n",
    "file_params['swift_provider'] = swift_provider\n",
    "file_params['swift_basename'] = swift_basename\n",
    "file_params['os_username'] = os_username\n",
    "file_params['os_tenant_name'] = os_tenant_name\n",
    "file_params['os_auth_url'] = os_auth_url\n",
    "file_params['os_password'] = os_password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from setupSpark import initSpark\n",
    "# Initialize Spark\n",
    "# specify the number of cores and the memory of the workers\n",
    "# each worker VM has 8 cores and 32 GB of memory\n",
    "# the status of the cluster (ie. how many cores are available) can be checked in the Spark UI:\n",
    "# http://SparkMasterIP:8080/\n",
    "\n",
    "spark_instances = 2 # the number of workers to be used\n",
    "executor_cores = 8 # the number of cores to be used on each worker\n",
    "executor_memory = '28G' # the amount of memory to be used on each worker\n",
    "max_cores = 16 # the max. number of cores Spark is allowed to use overall\n",
    "\n",
    "# returns the SparkContext object 'sc' which tells Spark how to access the cluster\n",
    "sc = initSpark(nbBackend, spark_instances=spark_instances, executor_cores=executor_cores, \\\n",
    "               max_cores=max_cores, executor_memory=executor_memory)\n",
    "\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# provide OS credentials to the Hadoop configuration\n",
    "sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.username', os_username)\n",
    "sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.tenant', os_tenant_name)\n",
    "sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.password', os_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add Python files in 'utils' folder to the SparkContext \n",
    "# this is required so that all files are available on all the cluster workers\n",
    "for filename in os.listdir(utils_dir):\n",
    "    if filename.endswith('.py'):\n",
    "        sc.addPyFile(os.path.join(utils_dir, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get HDF5 DFF files from Swift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check if container exists and return items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from SwiftStorageUtils import listItems\n",
    "object_list = listItems(swift_container, file_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narrow down the list to objects in pseudo-folder output_folder_dff containing filename_start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objects_to_download = [n for n in object_list if n.startswith(output_folder_dff) and filename_start in n]\n",
    "objects_to_download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create Spark RDD from list of objects to download and set the file stem as key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_rdd = sc.parallelize(objects_to_download)\n",
    "file_rdd = file_rdd.map(lambda x: (x.replace(output_folder_dff + '/', '').replace('.h5', ''), x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the functions for importing the data. As before, we first create a temporary folder on the local machine. Then we download the files into this temporary folder and read them with standard Python tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getArrayFromH5(h5file, dataset_name):\n",
    "    with h5py.File(h5file,'r') as hf:\n",
    "        print('List of arrays in HDF5 file: ', hf.keys())\n",
    "        data = hf.get(dataset_name)\n",
    "        data = np.array(data)\n",
    "        print('Shape of the array %s: ' % (dataset_name), data.shape)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from SwiftStorageUtils import downloadItems\n",
    "def convert2rdd(obj, file_params):\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    \n",
    "    # download options\n",
    "    down_opts = {\n",
    "        'skip_identical': True,\n",
    "        'out_directory': temp_dir,\n",
    "    }\n",
    "    downloadItems(file_params['swift_container'], [obj], file_params, down_opts)\n",
    "    \n",
    "    local_file = '%s%s%s' % (temp_dir, os.path.sep, obj)\n",
    "    print('Local file: ', local_file)\n",
    "    \n",
    "    data = getArrayFromH5(local_file, 'dff')\n",
    "    \n",
    "    # delete temp dir\n",
    "    shutil.rmtree(temp_dir)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we register this transformation to create dff_rdd from file_rdd. Here we only change the values and not the keys, so we can use mapValues instead of map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data from DFF files (HDF5 format)\n",
    "dff_rdd = file_rdd.mapValues(lambda v: convert2rdd(v, file_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we cache the new RDD (and there is sufficient space), subsequent steps might run a bit faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dff_rdd.persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the first element and determine image dimensions / number of frames. Then, setup time vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dff1 = dff_rdd.first()\n",
    "dims_analysis = (dff1[1].shape[0], dff1[1].shape[1])\n",
    "timepoints = dff1[1].shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time vector\n",
    "sample_rate = 20.0 # Hz\n",
    "t = (np.array(range(timepoints)) / sample_rate) - 3.0\n",
    "\n",
    "t_stim = -1.9 # stimulus cue (auditory)\n",
    "t_textIn = 0 # texture in (i.e. stimulus onset)\n",
    "t_textOut = 2 # texture starting to move out (stimulus offset)\n",
    "t_response = 4.9 # response cue for licking (auditory)\n",
    "t_base = -2 # baseline end (for F0 calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Rois and trial indices\n",
    "The approach for importing mat-files from Swift storage is the same as for HDF5 files: first download files from Swift storage to a temporary folder. Then, we use custom-written code to read the mat-files. Finally, the temporary folder is deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# File name of ROI mat-file\n",
    "roi_file = 'rois_OCIA.mat'\n",
    "# image dimensions on which coordinates in roi_file are based\n",
    "dims_roi = (256,256)\n",
    "# ROI names that should be extracted\n",
    "roi_dict = {'roi_S1BC': [], 'roi_A1': [], 'roi_EC': [], 'roi_M2': []}\n",
    "\n",
    "# File with trial indices\n",
    "trials_index_file = 'trials_ind.mat'\n",
    "\n",
    "# download and import Roi and trial index files\n",
    "objects_to_download = [\n",
    "    roi_file,\n",
    "    trials_index_file\n",
    "]\n",
    "# local storage directory --> remember to delete afterwards\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# download options\n",
    "down_opts = {\n",
    "    'skip_identical': True,\n",
    "    'out_directory': temp_dir,\n",
    "}\n",
    "\n",
    "from SwiftStorageUtils import downloadItems\n",
    "downloadItems(swift_container, objects_to_download, file_params, down_opts)\n",
    "\n",
    "trial_ind = wf.importTrialIndices('%s%s%s' % (temp_dir, os.path.sep, trials_index_file))\n",
    "\n",
    "# Specify ROIs to pull out\n",
    "roi_file = '%s%s%s' % (temp_dir, os.path.sep,roi_file)\n",
    "roi_dict = wf.importMatlabRois(roi_file, roi_dict, dims_roi, dims_analysis)\n",
    "\n",
    "# delete temp dir\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding-window cross-correlation\n",
    "Now we have everything in place for running the correlation analysis. First, we define a function (slidingWindowCorr) that computes the sliding window correlation between two vectors with a given window size. This function is optimised so that we only need to call the correlation function once. Next, we define a function to append NaNs to the beginning and end. Finally, we have a wrapper function that runs slidingWindowCorr for each pixel time series in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slidingWindowCorr(data1, data2, winsz):\n",
    "    \"\"\"\n",
    "    Calculate the cross-correlation between data1 and data2 with a moving window of winsz datapoints.\n",
    "    \"\"\"\n",
    "    N = int(data1.shape[0])\n",
    "    winsz_onesided = int(np.ceil(winsz/2))\n",
    "    # preallocate for efficiency\n",
    "    data1_windows = np.empty(shape=(N-winsz, winsz), dtype=np.float64)\n",
    "    data2_windows = np.empty(shape=(N-winsz, winsz), dtype=np.float64)\n",
    "    # build input arrays\n",
    "    row = 0\n",
    "    for t in range(winsz_onesided, N-winsz_onesided):\n",
    "        data1_windows[row, 0:winsz] = data1[t-winsz_onesided:t+winsz_onesided]\n",
    "        data2_windows[row, 0:winsz] = data2[t-winsz_onesided:t+winsz_onesided]\n",
    "        row += 1\n",
    "    # calculate correlation and extract relevant points in the correlation matrix\n",
    "    corr = np.corrcoef(np.vstack((data1_windows, data2_windows)))\n",
    "    ix1 = range(0,corr.shape[0]/2)\n",
    "    ix2 = range(corr.shape[0]/2, corr.shape[0])\n",
    "    corr = corr[ix1[:], ix2[:]]\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addNansToArray(A, winsz):\n",
    "    \"\"\"\n",
    "    add Nans at beginning and end\n",
    "    \"\"\"\n",
    "    nan_array = np.zeros((A.shape[0], A.shape[1], int(np.ceil(winsz/2))))\n",
    "    nan_array[:] = np.nan\n",
    "    return np.concatenate((nan_array, A, nan_array), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doSlidingWindowCorr(data, seed_pixel, winsz):\n",
    "    \"\"\"\n",
    "    Run sliding window correlation between all pixel timeseries in data and seed timeseries with winzs window size\n",
    "    \n",
    "    Return array with correlation values (same size as data)\n",
    "    \"\"\"\n",
    "    corr = np.zeros((data.shape[0], data.shape[1], data.shape[2]-winsz))\n",
    "    seed = np.nanmean(data[seed_pixel[0], seed_pixel[1], :], axis=0)\n",
    "    for x in range(data.shape[0]):\n",
    "        for y in range(data.shape[1]):\n",
    "            corr[x,y,:] = slidingWindowCorr(data[x,y,:], seed, winsz)\n",
    "    return addNansToArray(corr, winsz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can specify the window size (in frames), the seed ROI and then schedule the correlation analysis as RDD transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "winsz = 20\n",
    "seed_roi = 'roi_S1BC'\n",
    "\n",
    "corr_rdd = dff_rdd.mapValues(lambda v: doSlidingWindowCorr(v, roi_dict[seed_roi], winsz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we can look at the shape of the first element. It should match the dimensions of the input dFF arrays. This will run the analysis for the first element and take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr_rdd1 = corr_rdd.first()\n",
    "corr_rdd1[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save correlation results as HDF5 files\n",
    "Now we can save the data back to the Swift storage. This will finally kick-off the whole processing pipeline that has been defined so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include the seed ROI name in the output folder\n",
    "output_folder_corr = 'corr_%s' % (seed_roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the folders exist already. If a folder exists, will display the contents and ask for confirmation to delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from SwiftStorageUtils import deleteExistingFolder\n",
    "deleteExistingFolder(swift_container, output_folder_corr, file_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the correlation data as HDF5 on Swift storage. This will run all the transformations that have been registered for corr_rdd. Depending on the number of trials and image resolution, this may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from SwiftStorageUtils import saveAsH5\n",
    "corr_rdd.foreach(lambda (k,v): (k, saveAsH5(v, k, 'corr', output_folder_corr, file_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Correlation movie\n",
    "The final part of the notebook demonstrates how to make a movie out of the correlation arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select file to be displayed\n",
    "selected_file = '20152310_092225_4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import trial indices and figure out the trial type of the selected file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# download and import trial index files\n",
    "objects_to_download = [\n",
    "    trials_index_file\n",
    "]\n",
    "# local storage directory --> remember to delete afterwards\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# download options\n",
    "down_opts = {\n",
    "    'skip_identical': True,\n",
    "    'out_directory': temp_dir,\n",
    "}\n",
    "\n",
    "from SwiftStorageUtils import downloadItems\n",
    "downloadItems(swift_container, objects_to_download, file_params, down_opts)\n",
    "\n",
    "trial_ind = wf.importTrialIndices('%s%strials_ind.mat' % (temp_dir, os.path.sep))\n",
    "\n",
    "# delete temp dir\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTrialType(selected_file, trial_ind):\n",
    "    \"\"\"\n",
    "    Return trial type from of input file from trial_ind\n",
    "    \"\"\"\n",
    "    # parse file name to get trial_no\n",
    "    p = re.compile('\\d{1,8}')\n",
    "    file_info = p.findall(selected_file)\n",
    "    trial_no = int(file_info[2])\n",
    "    # search trial_ind for trial_type\n",
    "    trial_type = [i for i in trial_ind if trial_no in trial_ind[i]]\n",
    "    if not len(trial_type):\n",
    "        return 'void'\n",
    "    else:\n",
    "        return trial_type[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trial_type = getTrialType(selected_file, trial_ind)\n",
    "print('%s trial type: %s' % (selected_file, trial_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import correlation data from HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getArrayFromH5(h5file, dataset_name):\n",
    "    with h5py.File(h5file,'r') as hf:\n",
    "        print('List of arrays in HDF5 file: ', hf.keys())\n",
    "        data = hf.get(dataset_name)\n",
    "        data = np.array(data)\n",
    "        print('Shape of the array %s: ' % (dataset_name), data.shape)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# local storage directory --> remember to delete afterwards\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "# file to download from Swift\n",
    "objects_to_download = [\n",
    "    '%s/%s.h5' % (output_folder_corr, selected_file)\n",
    "]\n",
    "\n",
    "# download options\n",
    "down_opts = {\n",
    "    'skip_identical': True,\n",
    "    'out_directory': temp_dir,\n",
    "}\n",
    "\n",
    "# download file to local directory\n",
    "from SwiftStorageUtils import downloadItems\n",
    "downloadItems(swift_container, objects_to_download, file_params, down_opts)\n",
    "\n",
    "# read file from local directory\n",
    "corr_file = '%s%s%s%s%s.h5' % (temp_dir, os.path.sep, output_folder_corr, os.path.sep, selected_file)\n",
    "\n",
    "corr_data = getArrayFromH5(corr_file, 'corr')\n",
    "\n",
    "# delete temp dir\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and save the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wf.saveMovie(corr_data, trial_type, '%s_corr_%s' % (selected_file, seed_roi), sample_rate, t, file_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

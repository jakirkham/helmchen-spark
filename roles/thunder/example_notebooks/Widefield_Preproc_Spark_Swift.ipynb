{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess widefield calcium imaging data using Spark\n",
    "This notebook demonstrates how to read binary raw data files stored on UZH Swift object storage into a Spark RDD, convert it into a Numpy array and perform preprocessing to generate a DFF array. Both the raw data and DFF arrays are stored as output HDF5 files on the Swift object storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Python modules\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "from __future__ import print_function\n",
    "import getpass\n",
    "import h5py\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# the notebook backend: 'local' or 'openstack'\n",
    "nbBackend = 'openstack'\n",
    "\n",
    "# add folder 'utils' to the Python path\n",
    "# this folder contains custom written code that is required for data import and analysis\n",
    "utils_dir = os.path.join(os.getcwd(), 'utils')\n",
    "sys.path.append(utils_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import custom-written modules\n",
    "import SwiftStorageUtils\n",
    "import WidefieldDataUtils as wf\n",
    "import PickleUtils as pick\n",
    "import CalciumAnalysisUtils as calciumTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File paths and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start of name for matching files\n",
    "filename_start = '20152310_' # all files with names starting like this will be processed\n",
    "\n",
    "# swift file system\n",
    "swift_container = 'ariel' # specify name of container in Swift (do not use _ etc. in container names!)\n",
    "swift_provider = 'SparkTest' # in general, this should not change\n",
    "\n",
    "# derive the Swift base URI\n",
    "swift_basename = \"swift://\" + swift_container + \".\" + swift_provider + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OpenStack credentials for accessing Swift storage\n",
    "os_username = 'hluetc'\n",
    "os_tenant_name = 'helmchen.hifo.uzh'\n",
    "os_auth_url = 'https://cloud.s3it.uzh.ch:5000/v2.0'\n",
    "# provide OS password\n",
    "os_password = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put all these parameters in a dictionary, so that we can pass them conveniently to functions\n",
    "file_params = dict()\n",
    "file_params['filename_start'] = filename_start\n",
    "file_params['swift_container'] = swift_container\n",
    "file_params['swift_provider'] = swift_provider\n",
    "file_params['swift_basename'] = swift_basename\n",
    "file_params['os_username'] = os_username\n",
    "file_params['os_tenant_name'] = os_tenant_name\n",
    "file_params['os_auth_url'] = os_auth_url\n",
    "file_params['os_password'] = os_password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dimensions and number of frames of input data\n",
    "dims = (512,512)\n",
    "timepoints = 200\n",
    "\n",
    "# image dimensions for analysis (aspect ratio MUST be preserved)\n",
    "dims_analysis = (256,256) # use None or dims to skip resizing\n",
    "\n",
    "if not dims_analysis:\n",
    "    dims_analysis = dims\n",
    "\n",
    "# time vector and trial times\n",
    "sample_rate = 20.0 # Hz\n",
    "t = (np.array(range(timepoints)) / sample_rate) - 3.0\n",
    "\n",
    "t_stim = -1.9 # stimulus cue (auditory)\n",
    "t_textIn = 0 # texture in (i.e. stimulus onset)\n",
    "t_textOut = 2 # texture starting to move out (stimulus offset)\n",
    "t_response = 4.9 # response cue for licking (auditory)\n",
    "t_base = -2 # baseline end (for F0 calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bg_smooth = 30 # SD of Gaussian smoothing kernel for background estimation (in pixel) \n",
    "\n",
    "seg_cutoff = 0.0002 # Segmentation threshold; larger value = bigger mask; \n",
    "# smaller value = smaller mask (i.e. more pixels ignored); suggested = 0.0002\n",
    "\n",
    "# Frames for F0 calculation\n",
    "f0_frames = t<t_base # F0 as time before baseline\n",
    "\n",
    "f0_frames[:] = False\n",
    "f0_frames[9:12] = True # F0 as certain specified frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from setupSpark import initSpark\n",
    "# Initialize Spark\n",
    "# specify the number of cores and the memory of the workers\n",
    "# each worker VM has 8 cores and 32 GB of memory\n",
    "# the status of the cluster (ie. how many cores are available) can be checked in the Spark UI:\n",
    "# http://SparkMasterIP:8080/\n",
    "\n",
    "spark_instances = 2 # the number of workers to be used\n",
    "executor_cores = 8 # the number of cores to be used on each worker\n",
    "executor_memory = '28G' # the amount of memory to be used on each worker\n",
    "max_cores = 16 # the max. number of cores Spark is allowed to use overall\n",
    "\n",
    "# returns the SparkContext object 'sc' which tells Spark how to access the cluster\n",
    "sc = initSpark(nbBackend, spark_instances=spark_instances, executor_cores=executor_cores, \\\n",
    "               max_cores=max_cores, executor_memory=executor_memory)\n",
    "\n",
    "# from pyspark import SparkFiles, StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# provide OpenStack credentials to the Spark Hadoop configuration\n",
    "sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.username', os_username)\n",
    "sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.tenant', os_tenant_name)\n",
    "sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.password', os_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add Python files in 'utils' folder to the SparkContext \n",
    "# this is required so that all files are available on all the cluster workers\n",
    "for filename in os.listdir(utils_dir):\n",
    "    if filename.endswith('.py'):\n",
    "        sc.addPyFile(os.path.join(utils_dir, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load files into RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create Spark RDD with all objects in the Swift container\n",
    "file_rdd = sc.binaryFiles(file_params['swift_basename'])\n",
    "# filter relevant files\n",
    "file_rdd = file_rdd.filter(lambda (k,v): file_params['filename_start'] in k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements in file_rdd are key-value pairs, where the key is the file name and the value is the file's byte stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use count() to access every element in the RDD\n",
    "file_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert byte-stream to movie\n",
    "First, we define a function that specifies how the data should be read from a file. Then we perform an RDD transformation (map) that instructs Spark to pass the values of each element through the defined function. We also repartition the RDD to have as many partitions as number of cores. Note that RDD transfomations are executed only once they are actually needed ('lazy execution'). In this case, this will happen only when we want to return the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertDCAMtoMov(byte_stream):\n",
    "    \"\"\"\n",
    "    Convert raw DCAM byte-stream to movie. \n",
    "    \n",
    "    Note that parameters (e.g. image dimensions) are provided as global variables in the notebook.\n",
    "    \"\"\"\n",
    "    byte_stream = byte_stream[234:] # 234 bytes is the offset\n",
    "    A = np.fromstring(byte_stream, dtype=np.uint16)\n",
    "    A = A[:dims[0]*dims[1]*timepoints] # remove data points at the end\n",
    "    \n",
    "    # re-arrange data into the correct shape\n",
    "    mov = np.fliplr(A.reshape([dims[0], dims[1], timepoints], order='F'))\n",
    "    # hack to remove strange pixels with very high intensity\n",
    "    mov[np.where(mov > 60000)] = 0\n",
    "    \n",
    "    # resize to analysis dimensions\n",
    "    mov = wf.resizeMovie(mov, resolution=dims_analysis, interp='bilinear')\n",
    "    \n",
    "    return mov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert byte-stream to movie\n",
    "mov_rdd = file_rdd.map(lambda (k,v): (k, convertDCAMtoMov(v))).repartition(max_cores) # TODO: preserve keys\n",
    "# persist caches the RDD for faster access; for large RDDs, this may use a lot of memory\n",
    "# mov_rdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get first movie (return key-value tuple)\n",
    "mov1 = mov_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if the data has been imported correctly, display some frames as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path, file_id = os.path.split(mov1[0])\n",
    "print('File: %s' % (file_id))\n",
    "dat = mov1[1]\n",
    "xy = (dat.shape[0]/1.05, dat.shape[1] - (dat.shape[1]/1.1))\n",
    "f, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(dat[:,:,0], cmap='gray', interpolation='none')\n",
    "axes[0].annotate('Frame %1.0f' % 0, xy=xy, fontsize=14, color='yellow', horizontalalignment='right')\n",
    "axes[1].imshow(np.nanmean(dat, axis=2), cmap='gray', interpolation='none')\n",
    "axes[1].annotate('Mean', xy=xy, fontsize=14, color='yellow', horizontalalignment='right')\n",
    "axes[2].imshow(np.nanmax(dat, axis=2), cmap='gray', interpolation='none')\n",
    "axes[2].annotate('Max', xy=xy, fontsize=14, color='yellow', horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess movie\n",
    "The preprocessing pipeline currently consists of 3 steps: estimation and subtraction of background, segmentation of area of interest, normalization (dF/F calculation). As for conversion, we first define a function that is then applied to the Spark RDD. These transformations are only registered, not executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocMovie(mov, dims=dims, timepoints=timepoints, bg_smooth=bg_smooth, seg_cutoff=seg_cutoff):\n",
    "    \"\"\"\n",
    "    Perform preprocessing steps for a movie. \n",
    "    \"\"\"\n",
    "    \n",
    "    # estimate background signal intensity\n",
    "    print('Estimating background', end=\"\")\n",
    "    bg_estimate = wf.estimateBackground(mov[:,:,0], bg_smooth)\n",
    "    print(' - Done (%1.2f)' % bg_estimate)\n",
    "    \n",
    "    # subtract the background (set negative to 0)\n",
    "    mov = mov - bg_estimate\n",
    "    mov[mov<0] = 0\n",
    "    \n",
    "    # segment out the background (set to np.nan)\n",
    "    print('Segmenting background', end=\"\")\n",
    "    mov = wf.segmentBackground(mov, seg_cutoff, plot=False)\n",
    "    print(' - Done')\n",
    "    \n",
    "    # baseline normalization (Dff)\n",
    "    print('Calculating Dff', end=\"\")\n",
    "    dff = calciumTools.calculateDff(mov , f0_frames)\n",
    "    print(' - Done')\n",
    "    \n",
    "    return dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply transformation to the RDD\n",
    "dff_rdd = mov_rdd.map(lambda (k,v): (k, preprocMovie(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data as HDF5 files\n",
    "Now we can save the data back to the Swift storage. This will finally kick-off the whole processing pipeline that has been defined so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the names for the output folders\n",
    "output_folder_mov = 'mov_out'\n",
    "output_folder_dff = 'dff_out'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the folders exist already. If a folder exists, will display the contents and ask for confirmation to delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from SwiftStorageUtils import deleteExistingFolder\n",
    "deleteExistingFolder(swift_container, output_folder_mov, file_params)\n",
    "deleteExistingFolder(swift_container, output_folder_dff, file_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFileNameFromKey(key):\n",
    "    \"\"\"\n",
    "    Return the file name from the RDD key (i.e. split of the swift URL)\n",
    "    \"\"\"\n",
    "    path, name = os.path.split(key)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the image data as HDF5 on Swift storage. \n",
    "# This will run all the transformations that have been registered for mov_rdd.\n",
    "from SwiftStorageUtils import saveAsH5\n",
    "mov_rdd.foreach(lambda (k,v): (k, saveAsH5(v, getFileNameFromKey(k), 'mov', output_folder_mov, file_params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the dFF data as HDF5 on Swift storage. \n",
    "# This will run all the transformations that have been registered for dff_rdd.\n",
    "dff_rdd.foreach(lambda (k,v): (k, saveAsH5(v, getFileNameFromKey(k), 'dff', output_folder_dff, file_params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save RDD as pickle file (do NOT use for now!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save mov_rdd as pickle file on Swift\n",
    "# mov_rdd.saveAsPickleFile('%s%s' % (file_params['swift_basename'], output_folder_mov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check: load RDD and compare with original\n",
    "# mov_rdd_copy_swift = sc.pickleFile('swift://ariel.SparkTest/mov_out')\n",
    "# np.array_equal(mov_rdd.first()[1], mov_rdd_copy_swift.first()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save dff_rdd as pickle file on Swift\n",
    "# dff_rdd.saveAsPickleFile('%s%s' % (file_params['swift_basename'], output_folder_dff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
